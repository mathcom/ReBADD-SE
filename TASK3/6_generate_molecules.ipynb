{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96213a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import selfies as sf\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "from rdkit import RDLogger, Chem\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inter-op parallelism\n",
    "torch.set_num_interop_threads(4)\n",
    "torch.get_num_interop_threads()\n",
    "## Intra-op parallelism\n",
    "torch.set_num_threads(4)\n",
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ce90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUCONFIGS:\n",
    "    def __init__(self):\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device('cuda:0' if self.use_cuda else 'cpu')\n",
    "        if self.use_cuda: torch.cuda.set_device(self.device)\n",
    "        \n",
    "gpuconfigs = GPUCONFIGS()\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9037504",
   "metadata": {},
   "outputs": [],
   "source": [
    "REBADD_LIB_PATH = os.path.abspath(os.pardir)\n",
    "if REBADD_LIB_PATH not in sys.path:\n",
    "    sys.path = [REBADD_LIB_PATH] + sys.path\n",
    "\n",
    "from rebadd.stackVAE import StackAugmentedVAE\n",
    "from rebadd.datautils import GeneratorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReBADD_config import Reward_bcl2_bclxl_bclw as Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d224af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATACONFIGS:\n",
    "    def __init__(self):\n",
    "        ## input\n",
    "        self.input_dir = os.path.join('processed_data', 'zinc15')\n",
    "        self.train_data_path = os.path.join(self.input_dir, 'fragments_list.pkl')\n",
    "        self.vocab_data_path = os.path.join(self.input_dir, 'vocabulary.csv')\n",
    "        ## output\n",
    "        self.output_dir = os.path.join('outputs_6_generate_molecules', 'frag+reinforce+scst+offpolicy')\n",
    "        assert os.path.exists(self.output_dir)\n",
    "\n",
    "dataconfigs = DATACONFIGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfa26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data = GeneratorData(pickle_data_path=dataconfigs.train_data_path,\n",
    "                         vocabulary_path=dataconfigs.vocab_data_path,\n",
    "                         use_cuda=gpuconfigs.use_cuda)\n",
    "\n",
    "print(f\"Number of training samples: {len(gen_data.data)}\")\n",
    "print(f\"Number of vocabulary: {len(gen_data.vocabs)}\")\n",
    "print(f\"Maximum of seqlen: {gen_data.max_seqlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e68a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_ft = Reward(use_cuda=gpuconfigs.use_cuda, device=gpuconfigs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_generator = {\"input_size\"         : gen_data.n_characters,\n",
    "                    \"output_size\"        : gen_data.n_characters,\n",
    "                    \"max_seqlen\"         : 44,\n",
    "                    \"hidden_size\"        : 256,\n",
    "                    \"latent_size\"        : 64,\n",
    "                    \"n_layers\"           : 4,\n",
    "                    \"has_stack\"          : True,\n",
    "                    \"stack_width\"        : 256,\n",
    "                    \"stack_depth\"        : 20,\n",
    "                    \"lr\"                 : 1e-4,\n",
    "                    \"use_cuda\"           : gpuconfigs.use_cuda,\n",
    "                    \"device\"             : gpuconfigs.device,\n",
    "                    \"optimizer_instance\" : torch.optim.RMSprop}\n",
    "\n",
    "generator = StackAugmentedVAE(**kwargs_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a245c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CKPTCONFIGS:\n",
    "    def __init__(self):\n",
    "        self.input_dir = 'outputs_2_optimize_ReBADD'\n",
    "        self.modelnames = ['zinc15']\n",
    "        self.numbers = ['0500']\n",
    "        \n",
    "ckptconfigs = CKPTCONFIGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_SMILES(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    smi_rdkit = Chem.MolToSmiles(\n",
    "        mol,\n",
    "        isomericSmiles=False,   # modified because this option allows special tokens (e.g. [125I])\n",
    "        kekuleSmiles=False,     # default\n",
    "        rootedAtAtom=-1,        # default\n",
    "        canonical=True,         # default\n",
    "        allBondsExplicit=False, # default\n",
    "        allHsExplicit=False     # default\n",
    "    )\n",
    "    return smi_rdkit\n",
    "\n",
    "\n",
    "def generate_single_SMILES(data, generator, reward_ft, K, threshold):\n",
    "    best_smi = 'C'\n",
    "    best_rwd = threshold\n",
    "    \n",
    "    for _ in range(K):\n",
    "        ## SELFIES\n",
    "        z = generator.sample_latent_vectors()\n",
    "        sel = generator.evaluate(data, z=z, return_z=False, greedy=False)\n",
    "        sel = sel.replace(data.start_token, '').replace(data.end_token, '')\n",
    "        \n",
    "        ## SMILES\n",
    "        smi = sf.decoder(sel)\n",
    "    \n",
    "        ## Reward\n",
    "        try:\n",
    "            smi = normalize_SMILES(smi)\n",
    "            rwd = reward_ft(smi, return_min=True)\n",
    "        except:\n",
    "            rwd = threshold\n",
    "            \n",
    "        if rwd > best_rwd:\n",
    "            best_smi = smi\n",
    "            best_rwd = rwd\n",
    "        \n",
    "    return best_smi\n",
    "\n",
    "\n",
    "def generate_SMILES(data, generator, reward_ft, sample_size, K, threshold):\n",
    "    results = []\n",
    "    for _ in trange(sample_size):\n",
    "        best_smi = generate_single_SMILES(data, generator, reward_ft, K, threshold)\n",
    "        results.append(best_smi)\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_novel_SMILES(data, generator, reward_ft, sample_size, K, threshold, calc_sim):\n",
    "    results = []\n",
    "    for _ in trange(sample_size):\n",
    "        best_smi = 'C'\n",
    "        best_sim = 1.\n",
    "        for _ in range(K):\n",
    "            smi = generate_single_SMILES(data, generator, reward_ft, K, threshold)\n",
    "            sim = calc_sim(smi)\n",
    "            if sim < best_sim:\n",
    "                best_sim = sim\n",
    "                best_smi = smi\n",
    "        results.append(best_smi)\n",
    "    return results\n",
    "\n",
    "\n",
    "def SMILES_generate(data, generator, reward_ft, sample_size=5000, K=5, threshold=0., calc_sim=None):\n",
    "    generator.eval()\n",
    "    if calc_sim:\n",
    "        return generate_novel_SMILES(data, generator, reward_ft, sample_size, K, threshold, calc_sim)\n",
    "    else:\n",
    "        return generate_SMILES(data, generator, reward_ft, sample_size, K, threshold)\n",
    "    \n",
    "\n",
    "def save_smiles(filepath, smiles):\n",
    "    with open(filepath, 'w') as fout:\n",
    "        for smi in smiles:\n",
    "            fout.write(f\"{smi}\\n\")\n",
    "    print(f\"[INFO] {len(smiles)} SMILES were saved in {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sampling = 5000\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ed14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelname in ckptconfigs.modelnames:\n",
    "    for num in ckptconfigs.numbers:\n",
    "        \n",
    "        filepath = os.path.join(ckptconfigs.input_dir, modelname, f'checkpoint.pth.{num}')\n",
    "        generator.load_model(filepath)\n",
    "        \n",
    "        for k in range(K):\n",
    "            generated = SMILES_generate(gen_data, generator, reward_ft, sample_size=n_sampling)\n",
    "\n",
    "            save_smiles(os.path.join(dataconfigs.output_dir, f'smi_after.csv.{k}'), generated)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e8b594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
